---
title: "Lab 10 - Trees, Bagging, Random Forest"
output:
  html_document: default
  pdf_document: default
---

```{r setup}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging,and random forests for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with simulated data and the `heart` dataset that you can download from [here](https://github.com/JSC370/jsc370-2022/blob/main/data/heart/heart.csv)


### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval=FALSE, warning=FALSE, message = FALSE}
install.packages(c("rpart", "randomForest","gbm","xgboost"))
```

```{r, eval=FALSE, warning=FALSE, message = FALSE}
install.packages("rpart.plot")
```

### Load packages and data
```{r, message=FALSE, echo=TRUE, warning=FALSE, eval=TRUE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(kableExtra)
library(ggplot2)
library(dplyr)

heart<-read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/heart/heart.csv")

head(heart)
```


---

## Question 1: Trees with simulated data

- Simulate data from a random uniform distribution [-5,5] and normally distributed errors (s.d = 0.5)
- Create a non-linear relationship y=sin(x)+error
- Split the data into test and training sets (500 points each), plot the data

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
set.seed(1984)
n <- 1000
x <- runif(n, -5, 5)
error <- rnorm(n, sd = 0.5)
y <- sin(x) + error
nonlin <- data.frame(y = y, x = x)

train_size <- sample(1:1000, size = 500)
nonlin_train <- nonlin[train_size,]
nonlin_test <- nonlin[-train_size,]

nonlin %>%
  ggplot(aes(y = y, x = x)) +
  geom_point()
```

- Fit a regression tree using the training set, plot it

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
treefit <- rpart(y ~ x, method = "anova", control = list(cp = 0), data = nonlin_train)

rpart.plot(treefit)
```

- Determine the optimal complexity parameter (cp) to prune the tree

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
printcp(treefit)
optimalcp <- 0.00227267
```

- Prune and plot the tree and summarize

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
treepruned <- prune(treefit, cp = optimalcp)
summary(treepruned)
```

- Based on the plot and/or summary of the pruned tree create a vector of the (ordered) split points for variable x, and a vector of fitted values for the intervals determined by the split points of x.

```{r, echo=TRUE, eval=TRUE, warning=FALSE}
x_splits <- sort(treepruned$splits[, 'index'])
y_splits <- treepruned$frame[which(treepruned$frame[, 'var'] == '<leaf>'), 'yval']

```
- plot the step function corresponding to the fitted (pruned) tree
```{r, eval=TRUE, echo=TRUE, warning=FALSE}
plot(y ~ x, data = nonlin_train)
plot(stepfun(x_splits, y_splits), add = TRUE, col = 'red')
```

- Fit a linear model to the training data and plot the regression line. 
- Contrast the quality of the fit of the tree model vs. linear regression by inspection of the plot
- Compute the test MSE of the pruned tree and the linear regression model

```{r, echo=TRUE, eval=TRUE, warning=FALSE}
lmfit <- lm(y ~ x, data = nonlin_train)

plot(y ~ x)
abline(lmfit, col = "blue")
plot(stepfun(x_splits, y_splits), add = TRUE, col = "red")

tree_pred <- predict(treepruned, nonlin_test)
nonlin_test_tree <- cbind(nonlin_test, tree_pred)

tree_mse <- sum((nonlin_test_tree$tree_pred - nonlin_test_tree$y)^2) / dim(nonlin_test_tree)[1]
tree_mse

lm_pred <- predict(lmfit, nonlin_test)
nonlin_test_lm <- cbind(nonlin_test, lm_pred)
lm_mse <- sum((nonlin_test_lm$lm_pred - nonlin_test_lm$y)^2) / 500
lm_mse
```
- Is the lm or regression tree better at fitting a non-linear function?

From the above plot, we can see that the regression tree is a much better fit for fitting this non-linear function.

---

## Question 2: Analysis of Real Data

- Split the `heart` data into training and testing (70-30%)
```{r, echo=TRUE, eval=TRUE, warning=FALSE}
set.seed(1234)
train_idx <- sample(1:nrow(heart), round(0.7 * nrow(heart)))
train <- heart[train_idx,]
test <- heart[-train_idx,]
```

- Fit a classification tree using rpart, plot the full tree
```{r, echo=TRUE, eval=TRUE, warning=FALSE}
heart_tree <- rpart(AHD~., data = train, method = "class", control = list(minsplit = 10), minbucket = 3, cp = 0, xval = 10)

rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and prune the tree
```{r, echo=TRUE, eval=TRUE, warning=FALSE}
printcp(heart_tree)

optimalcp <- heart_tree$cptable[which.min(heart_tree$cptable[,"xerror"]), "CP"]
```

```{r, echo=TRUE, eval=TRUE, warning=FALSE}
heart_tree_prune <- prune(heart_tree, cp = optimalcp)

rpart.plot(heart_tree_prune)
```

- Compute the test misclassification error
```{r, echo=TRUE, eval=TRUE, warning=FALSE}
heart_pred <- predict(heart_tree_prune, test)
heart_pred <- as.data.frame(heart_pred)
heart_pred$AHD <- ifelse(heart_pred$Yes > 0.5, "yes", "no")

confmatrix_table <- table(true = test$AHD, predicted = heart_pred$AHD)
confmatrix_table

misclass_err <- (confmatrix_table[1, 2] + confmatrix_table[2, 1]) / nrow(test)
misclass_err
```

- Fit the tree with the optimal complexity parameter to the full data (training + testing)
```{r,echo=TRUE, eval=TRUE, warning=FALSE}
heart_tree <- rpart(AHD~., data = heart, method = "class", control = list(cp = optimalcp))
plotcp(heart_tree)
```
 - Out of Bag (OOB) error for tree
 
```{r,echo=TRUE, eval=TRUE, warning=FALSE}
min(heart_tree$cptable[,'xerror']) * nrow(heart)
```

---

## Question 3: Bagging, Random Forest

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 
- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r, echo=TRUE, eval=TRUE, warning=FALSE}
heart_bag <- randomForest(as.factor(AHD)~., data = train, mtry = 13, na.action = na.omit)
sum(heart_bag$err.rate[,1])

varImpPlot(heart_bag, n.var = 13, col = "red")

importance(heart_bag)
```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r, echo=TRUE, eval=TRUE, warning=FALSE}
heart_rf <- randomForest(as.factor(AHD)~., data = train, na.action = na.omit)

sum(heart_rf$err.rate[,1])

varImpPlot(heart_rf, n.var = 13, col = "green")

importance(heart_rf)
```


# Deliverables

1. Questions 1-4 answered, pdf or html output uploaded to quercus
